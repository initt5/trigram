{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b381c32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023a7434",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bbf7ab0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generator seed\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5009eb1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Char to idx maps\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)} # Characters to indexes\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()} # Indexes to characters\n",
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b42b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################### Bigram ####################################################################################\n",
    "\n",
    "# Split data set\n",
    "train_data, dev_data, test_data = random_split(words, [0.8, 0.1, 0.1], generator=g)\n",
    "\n",
    "def split_data(data):\n",
    "    X, y = [], []\n",
    "    for w in data:\n",
    "        chs = ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2 in zip(chs, chs[1:]):\n",
    "            X.append(stoi[ch1])\n",
    "            y.append(stoi[ch2])\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    return [X, y]\n",
    "\n",
    "# Create train, dev and test sets\n",
    "X_train, y_train = split_data(train_data)\n",
    "X_dev, y_dev = split_data(dev_data)\n",
    "X_test, y_test = split_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f160047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights matrix\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a914f0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 3.6793460845947266\n",
      "Epoch: 10 loss: 2.685643196105957\n",
      "Epoch: 20 loss: 2.571347713470459\n",
      "Epoch: 30 loss: 2.5294389724731445\n",
      "Epoch: 40 loss: 2.5084228515625\n",
      "Epoch: 50 loss: 2.496042251586914\n"
     ]
    }
   ],
   "source": [
    "n = X_train.nelement()\n",
    "\n",
    "# One hot encoding\n",
    "Xenc = F.one_hot(X_train, num_classes=27).float()\n",
    "\n",
    "# Gradient descent\n",
    "for epoch in range(50):\n",
    "    # Forward pass\n",
    "    logits = Xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(n), y_train].log().mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    W.data += -50 * W.grad\n",
    "    if (epoch == 0 or ((epoch+1)%10) == 0):\n",
    "        print(f'Epoch: {epoch+1} loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72c3d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_dev.nelement()\n",
    "\n",
    "# Loss on the dev set\n",
    "Xenc = F.one_hot(X_dev, num_classes=27).float()\n",
    "logits = Xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(n), y_dev].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1805f373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4942, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d20c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_test.nelement()\n",
    "\n",
    "# Loss on the test set\n",
    "Xenc = F.one_hot(X_test, num_classes=27).float()\n",
    "logits = Xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(n), y_test].log().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b255691b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4929, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a642d158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s.\n",
      "ma.\n",
      "aytin.\n",
      "mariahieslllyaadiannngu.\n",
      "asonn.\n"
     ]
    }
   ],
   "source": [
    "# Sample names\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5ac706",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################### Trigram ###################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2edcf830",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################# 1x54 vector approach ############################################################################\n",
    "# Split data set\n",
    "train_data, dev_data, test_data = random_split(words, [0.8, 0.1, 0.1], generator=g)\n",
    "\n",
    "def split_data(data):\n",
    "    X, y = [], []\n",
    "    for w in data:\n",
    "        chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            X.append((stoi[ch1], stoi[ch2]))\n",
    "            y.append(stoi[ch3])\n",
    "    return [X, y]\n",
    "\n",
    "# Create train, dev and test sets\n",
    "X_train, y_train = split_data(train_data)\n",
    "X_dev, y_dev = split_data(dev_data)\n",
    "X_test, y_test = split_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c79aa43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights matrix\n",
    "W = torch.randn((54, 27), requires_grad=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe6ba5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(X):\n",
    "    n = len(X)\n",
    "    Xenc = torch.zeros((n, 54))\n",
    "    for i in range(n):\n",
    "        vector = torch.zeros(54,)\n",
    "        ix1 = X[i][0]\n",
    "        ix2 = X[i][1]\n",
    "        vector[ix1] += 1\n",
    "        vector[27+ix2] += 1\n",
    "        Xenc[i] = vector\n",
    "    return Xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35eb8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xenc = one_hot_encode(X_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53df727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(W, epochs, lmbda, lr, tune_mode=False):\n",
    "    n = len(X_train)\n",
    "    \n",
    "    # Gradient descent\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = Xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        loss = -probs[torch.arange(n), y_train].log().mean() + lmbda*(W**2).mean()\n",
    "\n",
    "        # Backward pass\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Update\n",
    "        W.data += -lr * W.grad\n",
    "\n",
    "        if (tune_mode == False and (epoch == 0 or ((epoch+1)%10) == 0)):\n",
    "            print(f'Epoch: {epoch+1} train loss: {loss}')\n",
    "    if tune_mode:\n",
    "        print(f'Lambda: {lmbda}, train loss: {loss}')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89ef83a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0, train loss: 2.3869049549102783\n",
      "Lambda: 0, dev loss: 2.4121344089508057\n",
      "Lambda: 0.0001, train loss: 2.371954917907715\n",
      "Lambda: 0.0001, dev loss: 2.3981902599334717\n",
      "Lambda: 0.001, train loss: 2.3683440685272217\n",
      "Lambda: 0.001, dev loss: 2.3952150344848633\n",
      "Lambda: 0.005, train loss: 2.371345281600952\n",
      "Lambda: 0.005, dev loss: 2.3983840942382812\n",
      "Lambda: 0.01, train loss: 2.3766140937805176\n",
      "Lambda: 0.01, dev loss: 2.4035463333129883\n",
      "Lambda: 0.1, train loss: 2.44236159324646\n",
      "Lambda: 0.1, dev loss: 2.465195417404175\n"
     ]
    }
   ],
   "source": [
    "# Grid search for the best l2 regularization rate\n",
    "for lmbda in [0, 0.0001, 0.001, 0.005, 0.01, 0.1]:\n",
    "    temp_W = W\n",
    "    train_model(temp_W, 100, lmbda, 50, True)\n",
    "    n = len(X_dev)\n",
    "    xenc = one_hot_encode(X_dev).float()\n",
    "    \n",
    "    # Loss on the dev set\n",
    "    logits = xenc @ temp_W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(n), y_dev].log().mean() + lmbda * (W**2).mean()\n",
    "    print(f'Lambda: {lmbda}, dev loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f65529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 train loss: 2.4019176959991455\n",
      "Epoch: 10 train loss: 2.355705976486206\n",
      "Epoch: 20 train loss: 2.3540329933166504\n",
      "Epoch: 30 train loss: 2.3527488708496094\n",
      "Epoch: 40 train loss: 2.351701021194458\n",
      "Epoch: 50 train loss: 2.3508193492889404\n",
      "Epoch: 60 train loss: 2.3500618934631348\n",
      "Epoch: 70 train loss: 2.349400043487549\n",
      "Epoch: 80 train loss: 2.3488149642944336\n",
      "Epoch: 90 train loss: 2.348292589187622\n",
      "Epoch: 100 train loss: 2.3478219509124756\n"
     ]
    }
   ],
   "source": [
    "W = train_model(W, 100, 0.001, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37b7eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(X_test)\n",
    "Xenc = one_hot_encode(X_test).float()\n",
    "\n",
    "# Loss on the test set\n",
    "logits = Xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(n), y_test].log().mean() + 0.001 * (W**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98a7ee19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3564, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2057678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caishilezzvydes.\n",
      "maloh.\n",
      "haleo.\n",
      "aymi.\n",
      "ppusie.\n",
      "joh.\n",
      "cor.\n",
      "shtrama.\n",
      "rming.\n",
      "leynn.\n"
     ]
    }
   ],
   "source": [
    "# Sample names\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    # Start with ..\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    while True:\n",
    "        xenc = one_hot_encode([(ix1, ix2)])\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f57608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################# 1x729 vector approach ###########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ad5f002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data set\n",
    "train_data, dev_data, test_data = random_split(words, [0.8, 0.1, 0.1], generator=g)\n",
    "def split_data(data):\n",
    "    X, y = [], []\n",
    "    for w in data:\n",
    "        chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            X.append(stoi[ch1] + 27 * stoi[ch2])\n",
    "            y.append(stoi[ch3])\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    return [X, y]\n",
    "\n",
    "# Create train, dev and test sets\n",
    "X_train, y_train = split_data(train_data)\n",
    "X_dev, y_dev = split_data(dev_data)\n",
    "X_test, y_test = split_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bc602d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights matrix\n",
    "W = torch.randn((729, 27), requires_grad=True, generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ca25ba4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Xenc = F.one_hot(X_train, num_classes=729).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e30c07ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "def train_model(W, epochs, lmbda, lr, dev_mode=False):\n",
    "    n = X_train.nelement()\n",
    "    yenc = F.one_hot(y_train, num_classes=27).float()\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        logits = W[X_train, :] # one hot encoding\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims=True)\n",
    "        # loss = -probs[torch.arange(n), y_train].log().mean() + lmbda * (W**2).mean()  # <- nll\n",
    "        loss = F.cross_entropy(logits, y_train) + lmbda * (W**2).mean() # <- cross_entropy\n",
    "\n",
    "        # Backward pass\n",
    "        W.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Update\n",
    "        W.data += -lr * W.grad\n",
    "\n",
    "        if (not dev_mode and (epoch == 0 or ((epoch+1)%10) == 0)):\n",
    "            print(f'Epoch: {epoch+1} loss: {loss}')\n",
    "    if dev_mode:\n",
    "        print(f'Lambda: {lmbda}, train loss: {loss}')\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc8f2ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0, train loss: 2.2659521102905273\n",
      "Lambda: 0, dev loss: 2.245718479156494\n",
      "Lambda: 0.0001, train loss: 2.2400686740875244\n",
      "Lambda: 0.0001, dev loss: 2.224735736846924\n",
      "Lambda: 0.001, train loss: 2.23232102394104\n",
      "Lambda: 0.001, dev loss: 2.2195990085601807\n",
      "Lambda: 0.01, train loss: 2.2442729473114014\n",
      "Lambda: 0.01, dev loss: 2.2327253818511963\n",
      "Lambda: 0.1, train loss: 2.3420538902282715\n",
      "Lambda: 0.1, dev loss: 2.326735734939575\n",
      "Lambda: 0.5, train loss: 2.4763662815093994\n",
      "Lambda: 0.5, dev loss: 2.4524288177490234\n",
      "Lambda: 1, train loss: 2.57646107673645\n",
      "Lambda: 1, dev loss: 2.5503554344177246\n"
     ]
    }
   ],
   "source": [
    "for lmbda in [0, 0.0001, 0.001, 0.01, 0.1, 0.5, 1]:\n",
    "    temp_W = W\n",
    "    temp_W = train_model(temp_W, 300, lmbda, 150, dev_mode = True)\n",
    "    n = len(X_dev)\n",
    "    \n",
    "    # Loss on the dev set\n",
    "    logits = temp_W[X_dev, :] # one hot encoding\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    # loss = -probs[torch.arange(n), y_dev].log().mean() + lmbda * (temp_W**2).mean() <- nll\n",
    "    loss = F.cross_entropy(logits, y_dev) + lmbda * (W**2).mean() # <- cross entropy\n",
    "    \n",
    "    print(f'Lambda: {lmbda}, dev loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fbc0309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 loss: 2.3945083618164062\n",
      "Epoch: 10 loss: 2.366429090499878\n",
      "Epoch: 20 loss: 2.3605427742004395\n",
      "Epoch: 30 loss: 2.327146053314209\n",
      "Epoch: 40 loss: 2.331599235534668\n",
      "Epoch: 50 loss: 2.304323673248291\n",
      "Epoch: 60 loss: 2.31314754486084\n",
      "Epoch: 70 loss: 2.289116621017456\n",
      "Epoch: 80 loss: 2.300274610519409\n",
      "Epoch: 90 loss: 2.277801036834717\n",
      "Epoch: 100 loss: 2.2904279232025146\n",
      "Epoch: 110 loss: 2.269357919692993\n",
      "Epoch: 120 loss: 2.2828946113586426\n",
      "Epoch: 130 loss: 2.262455701828003\n",
      "Epoch: 140 loss: 2.2766668796539307\n",
      "Epoch: 150 loss: 2.2568814754486084\n",
      "Epoch: 160 loss: 2.2716007232666016\n",
      "Epoch: 170 loss: 2.2522597312927246\n",
      "Epoch: 180 loss: 2.2672817707061768\n",
      "Epoch: 190 loss: 2.248300075531006\n",
      "Epoch: 200 loss: 2.2636213302612305\n",
      "Epoch: 210 loss: 2.2448434829711914\n",
      "Epoch: 220 loss: 2.2605016231536865\n",
      "Epoch: 230 loss: 2.2418408393859863\n",
      "Epoch: 240 loss: 2.2577130794525146\n",
      "Epoch: 250 loss: 2.2391867637634277\n",
      "Epoch: 260 loss: 2.255222797393799\n",
      "Epoch: 270 loss: 2.2369132041931152\n",
      "Epoch: 280 loss: 2.2530670166015625\n",
      "Epoch: 290 loss: 2.234797477722168\n",
      "Epoch: 300 loss: 2.2510762214660645\n"
     ]
    }
   ],
   "source": [
    "W = train_model(W, 300, 0.001, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8da2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = X_test.nelement()\n",
    "\n",
    "# Loss on test set\n",
    "logits = W[X_test, :]\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "loss = -probs[torch.arange(n), y_test].log().mean() + 0.001 * (W**2).mean() # nll\n",
    "loss = F.cross_entropy(logits, y_test) + 0.001 * (W**2).mean() # cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2597e91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2596, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f547659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roshranadyssia.\n",
      "lisenn.\n",
      "le.\n",
      "elynn.\n",
      "ii.\n",
      "elleknisseaunuris.\n",
      "kri.\n",
      "shmarisarilaliddhagglxfzhhvi.\n",
      "ton.\n",
      "dsayli.\n"
     ]
    }
   ],
   "source": [
    "# Sample names\n",
    "for _ in range(10):\n",
    "    out = []\n",
    "    # Start with ..\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    while True:\n",
    "        ix = ix1 + 27 * ix2\n",
    "        logits = W[ix, :]\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum()\n",
    "        \n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7563a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Counting approach ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b5b0cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data set\n",
    "train_data, dev_data, test_data = random_split(words, [0.8, 0.1, 0.1], generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33d98baf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Trigram count matrix\n",
    "N = torch.zeros((27, 27, 27), dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe84d4a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filling in N matrix\n",
    "for w in train_data:\n",
    "    chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        ix3 = stoi[ch3]\n",
    "        N[ix1, ix2, ix3] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ff63b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative log likelihood\n",
    "def get_nll(NP, data):\n",
    "    log_likelihood = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for w in data:\n",
    "        chs = ['.'] + ['.'] + list(w) + ['.']\n",
    "        for ch1, ch2, ch3 in zip(chs, chs[1:], chs[2:]):\n",
    "            ix1 = stoi[ch1]\n",
    "            ix2 = stoi[ch2]\n",
    "            ix3 = stoi[ch3]\n",
    "            prob = NP[ix1, ix2, ix3]\n",
    "            logprob = torch.log(prob)\n",
    "            log_likelihood += logprob # add log prob of next character\n",
    "            n += 1\n",
    "    nll = -log_likelihood\n",
    "    return nll/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9022c1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing rate: 0, train loss: 2.1816229820251465, dev loss: nan\n",
      "Smoothing rate: 1, train loss: 2.2142655849456787, dev loss: 2.2413170337677\n",
      "Smoothing rate: 2, train loss: 2.2375996112823486, dev loss: 2.260141611099243\n",
      "Smoothing rate: 3, train loss: 2.257136821746826, dev loss: 2.2772293090820312\n",
      "Smoothing rate: 5, train loss: 2.2905335426330566, dev loss: 2.3071930408477783\n",
      "Smoothing rate: 10, train loss: 2.354750633239746, dev loss: 2.367098569869995\n",
      "Smoothing rate: 15, train loss: 2.4044113159179688, dev loss: 2.4140870571136475\n",
      "Smoothing rate: 20, train loss: 2.445242404937744, dev loss: 2.4532060623168945\n",
      "Smoothing rate: 50, train loss: 2.604304313659668, dev loss: 2.6079306602478027\n"
     ]
    }
   ],
   "source": [
    "# Grid search for the best smoothing rate\n",
    "for sr in [0, 1, 2, 3, 5, 10, 15, 20, 50]:\n",
    "    NP = (N+sr).float()\n",
    "    NP = NP / NP.sum(2, keepdims=True)\n",
    "    nll_train = get_nll(NP, train_data)\n",
    "    nll_dev = get_nll(NP, dev_data)\n",
    "    print(f'Smoothing rate: {sr}, train loss: {nll_train}, dev loss: {nll_dev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2d123f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NP = (N+1).float()\n",
    "NP = NP / NP.sum(2, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f030ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss: 2.2442703247070312\n"
     ]
    }
   ],
   "source": [
    "# Test set loss\n",
    "nll_test = get_nll(NP, test_data)\n",
    "print(f'test loss: {nll_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c23c2f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base.\n",
      "azia.\n",
      "eli.\n",
      "marikhrittene.\n",
      "samanephiytren.\n"
     ]
    }
   ],
   "source": [
    "# Sample names\n",
    "for _ in range(5):\n",
    "    out = []\n",
    "    ix1 = 0\n",
    "    ix2 = 0\n",
    "    while True:\n",
    "        p = NP[ix1][ix2]\n",
    "        ix1 = ix2\n",
    "        ix2 = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item() # Sample next character from NP\n",
    "        out.append(itos[ix2])\n",
    "        if ix2 == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2f8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
